{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. DATASET 준비\n",
        "\n",
        "이번 과제는 전세계에서 가장 큰 인공지능 오픈소스 커뮤니티인 hugging face의 dataset을 기반으로 학습을 진행해볼 것이기 때문에, hugging face에서 제공하는 datasets 라이브러리를 통해 간편하게 받아올 수 있다."
      ],
      "metadata": {
        "id": "OKqoLwKdRM3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG7h25FBy5uL",
        "outputId": "45203d25-6465-4509-a3db-2eef9aa0fdf0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. DATA 전처리 및 DATACLASS 정의\n",
        "\n",
        "DATA를 로드한 후 다루기 쉽게 pandas의 DATAFRAME 형태로 변경한 뒤, 해당 데이터를 train : val : test = 0.8 : 0.1 : 0.1 로 나눠준다. 더불어 DATASET 클래스를 정의하고, DATALOADER에서 불러올 수 있도록 한다."
      ],
      "metadata": {
        "id": "nbV8RE4dSa8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "favzdBf5w2ae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "# Dataset loading and processing\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "eth_data = load_dataset(\"StockLlama/ETH-USD-2022-01-01_2024-08-30\")\n",
        "\n",
        "df = pd.DataFrame(eth_data['train'])\n",
        "\n",
        "data_inputs = np.array(df['input_ids'].to_list())\n",
        "labels = np.array(df['label'].values)\n",
        "\n",
        "# Define hyperparameters\n",
        "sequence_length = 50\n",
        "batch_size = 32\n",
        "epochs = 150\n",
        "learning_rate = 0.001\n",
        "hidden_size = 64\n",
        "\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "train_size = int(len(data_inputs) * 0.8)\n",
        "train_data = data_inputs[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "\n",
        "# train_data = (train_data - np.mean(train_data)) / np.std(train_data)\n",
        "# trian_labels = (train_labels - np.mean(train_labels)) / np.std(train_labels)\n",
        "\n",
        "test_data = data_inputs[train_size:]\n",
        "test_labels = labels[train_size:]\n",
        "\n",
        "# test_data = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "# test_labels = (test_labels - np.mean(test_labels)) / np.std(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.labels[index]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "Pp3CwA-7OK85"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TimeSeriesDataset(train_data, train_labels)\n",
        "test_dataset = TimeSeriesDataset(test_data, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        print(x_batch.unsqueeze(-1).shape)\n",
        "        print(x_batch.shape, y_batch.shape)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxPasMAIPXbG",
        "outputId": "6586287b-17bd-4872-82fe-19f5b1e4e851"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 256, 1])\n",
            "torch.Size([32, 256]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 모델 정의\n",
        "\n",
        "총 3가지의 모델을 정의할 것인데, LSTM, RNN, GRU 이다. 세모델 모두 시계열 정보를 표현하기 위해서 사용되며, LSTM과 GRU 는 RNN의 확장판으로 longterm dependency를 보완하기 위해 gate가 추가된 형태의 모델들이다."
      ],
      "metadata": {
        "id": "kauy-YO_TLxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "RUoWYSNqOn9F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "50c0TF3ZOpg_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZlNob7UZOrYC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 모델 학습\n",
        "이전 다른 모든 과제들과 동일하게 학습 코드를 작성했으며, 매 epoch 당 train, test accuracy를 구하도록 구성했다."
      ],
      "metadata": {
        "id": "hJrwkQiBTvQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(x_batch).squeeze()\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            predictions = model(x_batch).squeeze()\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "models = {\n",
        "    \"RNN\": RNNModel(1, hidden_size).to(device),\n",
        "    \"LSTM\": LSTMModel(1, hidden_size).to(device),\n",
        "    \"GRU\": GRUModel(1, hidden_size).to(device)\n",
        "}\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    results[model_name] = (train_losses, test_losses)\n",
        "\n",
        "    plt.plot(train_losses, label=f'{model_name} Train Loss')\n",
        "    plt.plot(test_losses, label=f'{model_name} Test Loss')\n",
        "\n",
        "plt.title('Training and Test Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDhaMDkgY9DI",
        "outputId": "58d8bbab-85d2-4c95-be02-da4581c578b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN model...\n",
            "Epoch 1/150, Train Loss: 1579.6986, Test Loss: 5229.3249\n",
            "Epoch 2/150, Train Loss: 1458.3036, Test Loss: 5031.3942\n",
            "Epoch 3/150, Train Loss: 1347.5934, Test Loss: 4795.8609\n",
            "Epoch 4/150, Train Loss: 1262.7355, Test Loss: 4610.2062\n",
            "Epoch 5/150, Train Loss: 1189.5648, Test Loss: 4453.9587\n",
            "Epoch 6/150, Train Loss: 1125.5056, Test Loss: 4307.5049\n",
            "Epoch 7/150, Train Loss: 1061.2652, Test Loss: 4167.3279\n",
            "Epoch 8/150, Train Loss: 1002.2608, Test Loss: 4031.8334\n",
            "Epoch 9/150, Train Loss: 947.1406, Test Loss: 3903.8603\n",
            "Epoch 10/150, Train Loss: 897.4433, Test Loss: 3780.0753\n",
            "Epoch 11/150, Train Loss: 847.7368, Test Loss: 3660.3444\n",
            "Epoch 12/150, Train Loss: 804.2970, Test Loss: 3547.3972\n",
            "Epoch 13/150, Train Loss: 764.7528, Test Loss: 3442.2159\n",
            "Epoch 14/150, Train Loss: 723.5269, Test Loss: 3335.5025\n",
            "Epoch 15/150, Train Loss: 687.7630, Test Loss: 3235.4584\n",
            "Epoch 16/150, Train Loss: 652.8901, Test Loss: 3141.0462\n",
            "Epoch 17/150, Train Loss: 622.1579, Test Loss: 3051.4421\n",
            "Epoch 18/150, Train Loss: 595.4083, Test Loss: 2963.2725\n",
            "Epoch 19/150, Train Loss: 567.9087, Test Loss: 2880.4898\n",
            "Epoch 20/150, Train Loss: 542.2065, Test Loss: 2800.3771\n",
            "Epoch 21/150, Train Loss: 519.0237, Test Loss: 2724.2830\n",
            "Epoch 22/150, Train Loss: 499.3801, Test Loss: 2650.1249\n",
            "Epoch 23/150, Train Loss: 479.2481, Test Loss: 2580.2866\n",
            "Epoch 24/150, Train Loss: 461.1868, Test Loss: 2517.2187\n",
            "Epoch 25/150, Train Loss: 443.7459, Test Loss: 2454.0449\n",
            "Epoch 26/150, Train Loss: 428.5459, Test Loss: 2392.4366\n",
            "Epoch 27/150, Train Loss: 407.7352, Test Loss: 2333.4333\n",
            "Epoch 28/150, Train Loss: 391.5965, Test Loss: 2273.0066\n",
            "Epoch 29/150, Train Loss: 375.6729, Test Loss: 2214.7485\n",
            "Epoch 30/150, Train Loss: 360.5688, Test Loss: 2158.6583\n",
            "Epoch 31/150, Train Loss: 349.3339, Test Loss: 2104.4815\n",
            "Epoch 32/150, Train Loss: 336.3917, Test Loss: 2054.5169\n",
            "Epoch 33/150, Train Loss: 325.0355, Test Loss: 2004.0742\n",
            "Epoch 34/150, Train Loss: 316.9348, Test Loss: 1956.0927\n",
            "Epoch 35/150, Train Loss: 306.6226, Test Loss: 1908.6216\n",
            "Epoch 36/150, Train Loss: 297.1723, Test Loss: 1864.7496\n",
            "Epoch 37/150, Train Loss: 285.6989, Test Loss: 1819.3435\n",
            "Epoch 38/150, Train Loss: 277.9637, Test Loss: 1776.4830\n",
            "Epoch 39/150, Train Loss: 271.1711, Test Loss: 1737.4371\n",
            "Epoch 40/150, Train Loss: 262.3319, Test Loss: 1696.8596\n",
            "Epoch 41/150, Train Loss: 255.2909, Test Loss: 1656.6439\n",
            "Epoch 42/150, Train Loss: 249.7167, Test Loss: 1620.4919\n",
            "Epoch 43/150, Train Loss: 241.0341, Test Loss: 1584.2613\n",
            "Epoch 44/150, Train Loss: 237.1847, Test Loss: 1548.3547\n",
            "Epoch 45/150, Train Loss: 229.3065, Test Loss: 1514.0459\n",
            "Epoch 46/150, Train Loss: 224.1423, Test Loss: 1481.2291\n",
            "Epoch 47/150, Train Loss: 217.6615, Test Loss: 1448.7277\n",
            "Epoch 48/150, Train Loss: 212.7411, Test Loss: 1417.1255\n",
            "Epoch 49/150, Train Loss: 207.1248, Test Loss: 1385.3282\n",
            "Epoch 50/150, Train Loss: 202.3766, Test Loss: 1355.6129\n",
            "Epoch 51/150, Train Loss: 198.5533, Test Loss: 1326.7046\n",
            "Epoch 52/150, Train Loss: 193.0471, Test Loss: 1296.1205\n",
            "Epoch 53/150, Train Loss: 188.7778, Test Loss: 1267.1656\n",
            "Epoch 54/150, Train Loss: 184.2131, Test Loss: 1239.9085\n",
            "Epoch 55/150, Train Loss: 180.3170, Test Loss: 1213.4660\n",
            "Epoch 56/150, Train Loss: 175.5642, Test Loss: 1185.9268\n",
            "Epoch 57/150, Train Loss: 171.0965, Test Loss: 1159.7215\n",
            "Epoch 58/150, Train Loss: 168.8918, Test Loss: 1134.9317\n",
            "Epoch 59/150, Train Loss: 163.5352, Test Loss: 1109.1929\n",
            "Epoch 60/150, Train Loss: 160.1014, Test Loss: 1084.6982\n",
            "Epoch 61/150, Train Loss: 156.9892, Test Loss: 1060.4395\n",
            "Epoch 62/150, Train Loss: 153.3083, Test Loss: 1037.7232\n",
            "Epoch 63/150, Train Loss: 150.0692, Test Loss: 1014.3602\n",
            "Epoch 64/150, Train Loss: 145.9382, Test Loss: 991.0121\n",
            "Epoch 65/150, Train Loss: 143.1979, Test Loss: 970.5734\n",
            "Epoch 66/150, Train Loss: 141.6581, Test Loss: 949.1610\n",
            "Epoch 67/150, Train Loss: 136.8210, Test Loss: 926.2531\n",
            "Epoch 68/150, Train Loss: 133.8692, Test Loss: 906.5132\n",
            "Epoch 69/150, Train Loss: 131.0224, Test Loss: 885.9478\n",
            "Epoch 70/150, Train Loss: 129.2599, Test Loss: 865.6447\n",
            "Epoch 71/150, Train Loss: 125.5837, Test Loss: 847.1639\n",
            "Epoch 72/150, Train Loss: 123.0123, Test Loss: 828.0136\n",
            "Epoch 73/150, Train Loss: 120.7770, Test Loss: 809.9404\n",
            "Epoch 74/150, Train Loss: 118.3636, Test Loss: 791.1138\n",
            "Epoch 75/150, Train Loss: 115.5819, Test Loss: 772.6904\n",
            "Epoch 76/150, Train Loss: 112.9001, Test Loss: 756.4982\n",
            "Epoch 77/150, Train Loss: 111.9937, Test Loss: 740.0202\n",
            "Epoch 78/150, Train Loss: 109.4175, Test Loss: 724.1597\n",
            "Epoch 79/150, Train Loss: 106.6300, Test Loss: 707.0280\n",
            "Epoch 80/150, Train Loss: 104.7109, Test Loss: 690.9625\n",
            "Epoch 81/150, Train Loss: 103.9976, Test Loss: 676.9459\n",
            "Epoch 82/150, Train Loss: 101.0266, Test Loss: 661.2690\n",
            "Epoch 83/150, Train Loss: 98.6340, Test Loss: 646.4641\n",
            "Epoch 84/150, Train Loss: 97.3916, Test Loss: 631.4371\n",
            "Epoch 85/150, Train Loss: 95.4105, Test Loss: 617.6592\n",
            "Epoch 86/150, Train Loss: 93.6656, Test Loss: 602.4415\n",
            "Epoch 87/150, Train Loss: 91.5528, Test Loss: 588.4951\n",
            "Epoch 88/150, Train Loss: 89.5299, Test Loss: 574.8088\n",
            "Epoch 89/150, Train Loss: 87.2635, Test Loss: 561.1742\n",
            "Epoch 90/150, Train Loss: 86.0789, Test Loss: 549.0379\n",
            "Epoch 91/150, Train Loss: 84.0433, Test Loss: 535.5616\n",
            "Epoch 92/150, Train Loss: 82.8193, Test Loss: 522.4092\n",
            "Epoch 93/150, Train Loss: 81.0834, Test Loss: 511.1440\n",
            "Epoch 94/150, Train Loss: 79.0050, Test Loss: 498.3862\n",
            "Epoch 95/150, Train Loss: 77.1803, Test Loss: 486.1213\n",
            "Epoch 96/150, Train Loss: 75.8407, Test Loss: 474.8237\n",
            "Epoch 97/150, Train Loss: 74.4808, Test Loss: 462.8392\n",
            "Epoch 98/150, Train Loss: 72.7939, Test Loss: 452.2618\n",
            "Epoch 99/150, Train Loss: 71.0348, Test Loss: 441.2219\n",
            "Epoch 100/150, Train Loss: 69.6272, Test Loss: 430.0394\n",
            "Epoch 101/150, Train Loss: 68.6017, Test Loss: 420.6071\n",
            "Epoch 102/150, Train Loss: 66.5790, Test Loss: 409.9790\n",
            "Epoch 103/150, Train Loss: 65.9864, Test Loss: 399.8411\n",
            "Epoch 104/150, Train Loss: 63.8638, Test Loss: 390.0805\n",
            "Epoch 105/150, Train Loss: 62.3688, Test Loss: 380.2314\n",
            "Epoch 106/150, Train Loss: 61.8529, Test Loss: 371.4663\n",
            "Epoch 107/150, Train Loss: 60.1331, Test Loss: 362.1108\n",
            "Epoch 108/150, Train Loss: 58.9668, Test Loss: 352.8971\n",
            "Epoch 109/150, Train Loss: 57.9614, Test Loss: 343.5309\n",
            "Epoch 110/150, Train Loss: 56.4526, Test Loss: 334.7155\n",
            "Epoch 111/150, Train Loss: 54.9705, Test Loss: 326.0272\n",
            "Epoch 112/150, Train Loss: 53.7899, Test Loss: 317.8116\n",
            "Epoch 113/150, Train Loss: 52.6418, Test Loss: 309.7443\n",
            "Epoch 114/150, Train Loss: 51.8690, Test Loss: 301.6465\n",
            "Epoch 115/150, Train Loss: 50.5944, Test Loss: 293.8663\n",
            "Epoch 116/150, Train Loss: 49.1624, Test Loss: 286.5751\n",
            "Epoch 117/150, Train Loss: 48.0678, Test Loss: 278.9223\n",
            "Epoch 118/150, Train Loss: 47.2766, Test Loss: 271.8621\n",
            "Epoch 119/150, Train Loss: 46.1737, Test Loss: 264.6228\n",
            "Epoch 120/150, Train Loss: 44.8949, Test Loss: 257.2041\n",
            "Epoch 121/150, Train Loss: 44.0439, Test Loss: 250.6889\n",
            "Epoch 122/150, Train Loss: 42.8573, Test Loss: 244.5466\n",
            "Epoch 123/150, Train Loss: 41.9271, Test Loss: 237.7400\n",
            "Epoch 124/150, Train Loss: 41.0477, Test Loss: 231.3859\n",
            "Epoch 125/150, Train Loss: 40.5916, Test Loss: 225.6964\n",
            "Epoch 126/150, Train Loss: 39.2010, Test Loss: 219.4006\n",
            "Epoch 127/150, Train Loss: 38.4699, Test Loss: 213.1585\n",
            "Epoch 128/150, Train Loss: 37.4908, Test Loss: 207.5512\n",
            "Epoch 129/150, Train Loss: 36.8908, Test Loss: 202.1760\n",
            "Epoch 130/150, Train Loss: 36.2299, Test Loss: 196.8400\n",
            "Epoch 131/150, Train Loss: 35.0769, Test Loss: 191.2740\n",
            "Epoch 132/150, Train Loss: 34.7940, Test Loss: 186.2822\n",
            "Epoch 133/150, Train Loss: 33.6362, Test Loss: 180.7061\n",
            "Epoch 134/150, Train Loss: 32.6170, Test Loss: 176.3818\n",
            "Epoch 135/150, Train Loss: 32.0867, Test Loss: 171.6231\n",
            "Epoch 136/150, Train Loss: 31.1476, Test Loss: 166.2114\n",
            "Epoch 137/150, Train Loss: 30.4711, Test Loss: 161.8424\n",
            "Epoch 138/150, Train Loss: 29.5750, Test Loss: 156.6862\n",
            "Epoch 139/150, Train Loss: 28.9292, Test Loss: 152.4631\n",
            "Epoch 140/150, Train Loss: 28.4160, Test Loss: 148.9904\n",
            "Epoch 141/150, Train Loss: 27.8471, Test Loss: 144.1047\n",
            "Epoch 142/150, Train Loss: 27.2176, Test Loss: 140.2985\n",
            "Epoch 143/150, Train Loss: 26.6730, Test Loss: 136.1954\n",
            "Epoch 144/150, Train Loss: 26.0067, Test Loss: 132.7459\n",
            "Epoch 145/150, Train Loss: 25.0821, Test Loss: 128.8166\n",
            "Epoch 146/150, Train Loss: 24.6467, Test Loss: 125.1366\n",
            "Epoch 147/150, Train Loss: 23.9402, Test Loss: 121.9898\n",
            "Epoch 148/150, Train Loss: 23.4731, Test Loss: 118.4659\n",
            "Epoch 149/150, Train Loss: 23.0361, Test Loss: 115.6694\n",
            "Epoch 150/150, Train Loss: 22.4568, Test Loss: 112.5988\n",
            "Training LSTM model...\n",
            "Epoch 1/150, Train Loss: 1576.6840, Test Loss: 5268.9860\n",
            "Epoch 2/150, Train Loss: 1489.8813, Test Loss: 5108.5892\n",
            "Epoch 3/150, Train Loss: 1377.8888, Test Loss: 4859.9366\n",
            "Epoch 4/150, Train Loss: 1250.2047, Test Loss: 4556.8864\n",
            "Epoch 5/150, Train Loss: 1148.4878, Test Loss: 4341.1259\n",
            "Epoch 6/150, Train Loss: 1065.3517, Test Loss: 4153.4349\n",
            "Epoch 7/150, Train Loss: 973.4377, Test Loss: 3921.6516\n",
            "Epoch 8/150, Train Loss: 895.0513, Test Loss: 3751.0840\n",
            "Epoch 9/150, Train Loss: 828.4767, Test Loss: 3584.5386\n",
            "Epoch 10/150, Train Loss: 767.7485, Test Loss: 3435.9176\n",
            "Epoch 11/150, Train Loss: 719.5567, Test Loss: 3304.4447\n",
            "Epoch 12/150, Train Loss: 670.6599, Test Loss: 3180.6747\n",
            "Epoch 13/150, Train Loss: 634.0488, Test Loss: 3069.0412\n",
            "Epoch 14/150, Train Loss: 597.4372, Test Loss: 2963.2121\n",
            "Epoch 15/150, Train Loss: 566.3303, Test Loss: 2865.1750\n",
            "Epoch 16/150, Train Loss: 537.5315, Test Loss: 2771.5542\n",
            "Epoch 17/150, Train Loss: 508.8704, Test Loss: 2682.7520\n",
            "Epoch 18/150, Train Loss: 483.3076, Test Loss: 2598.6556\n",
            "Epoch 19/150, Train Loss: 458.9811, Test Loss: 2518.3476\n",
            "Epoch 20/150, Train Loss: 435.6430, Test Loss: 2439.4777\n",
            "Epoch 21/150, Train Loss: 415.2107, Test Loss: 2367.3376\n",
            "Epoch 22/150, Train Loss: 396.4302, Test Loss: 2296.2421\n",
            "Epoch 23/150, Train Loss: 378.6169, Test Loss: 2229.5938\n",
            "Epoch 24/150, Train Loss: 365.2434, Test Loss: 2163.9658\n",
            "Epoch 25/150, Train Loss: 350.4729, Test Loss: 2103.0013\n",
            "Epoch 26/150, Train Loss: 335.1178, Test Loss: 2045.1579\n",
            "Epoch 27/150, Train Loss: 323.1876, Test Loss: 1989.1747\n",
            "Epoch 28/150, Train Loss: 310.8356, Test Loss: 1933.8288\n",
            "Epoch 29/150, Train Loss: 299.6398, Test Loss: 1882.9029\n",
            "Epoch 30/150, Train Loss: 288.6936, Test Loss: 1832.1908\n",
            "Epoch 31/150, Train Loss: 280.1405, Test Loss: 1784.0802\n",
            "Epoch 32/150, Train Loss: 271.2941, Test Loss: 1738.4712\n",
            "Epoch 33/150, Train Loss: 263.9383, Test Loss: 1695.9670\n",
            "Epoch 34/150, Train Loss: 255.0205, Test Loss: 1651.5742\n",
            "Epoch 35/150, Train Loss: 247.5155, Test Loss: 1611.3845\n",
            "Epoch 36/150, Train Loss: 240.9770, Test Loss: 1570.4711\n",
            "Epoch 37/150, Train Loss: 232.4484, Test Loss: 1531.2827\n",
            "Epoch 38/150, Train Loss: 226.6234, Test Loss: 1493.4635\n",
            "Epoch 39/150, Train Loss: 220.0256, Test Loss: 1456.6296\n",
            "Epoch 40/150, Train Loss: 214.7300, Test Loss: 1420.9933\n",
            "Epoch 41/150, Train Loss: 207.2031, Test Loss: 1385.9161\n",
            "Epoch 42/150, Train Loss: 202.2844, Test Loss: 1353.4135\n",
            "Epoch 43/150, Train Loss: 197.2553, Test Loss: 1319.7743\n",
            "Epoch 44/150, Train Loss: 190.9968, Test Loss: 1286.2557\n",
            "Epoch 45/150, Train Loss: 187.5344, Test Loss: 1255.5196\n",
            "Epoch 46/150, Train Loss: 182.6485, Test Loss: 1226.1592\n",
            "Epoch 47/150, Train Loss: 176.4074, Test Loss: 1195.3549\n",
            "Epoch 48/150, Train Loss: 174.0635, Test Loss: 1166.1065\n",
            "Epoch 49/150, Train Loss: 168.1486, Test Loss: 1137.2246\n",
            "Epoch 50/150, Train Loss: 164.1922, Test Loss: 1109.4083\n",
            "Epoch 51/150, Train Loss: 160.0785, Test Loss: 1081.9886\n",
            "Epoch 52/150, Train Loss: 156.2691, Test Loss: 1055.6470\n",
            "Epoch 53/150, Train Loss: 151.6123, Test Loss: 1029.7515\n",
            "Epoch 54/150, Train Loss: 148.4247, Test Loss: 1004.6543\n",
            "Epoch 55/150, Train Loss: 145.0735, Test Loss: 979.9160\n",
            "Epoch 56/150, Train Loss: 141.7210, Test Loss: 955.1740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 분석 및 고찰\n",
        "\n",
        "## **ETH-USD 시계열 데이터**\n",
        "\n",
        "이 코드에서 사용하는 데이터셋은 Hugging Face의 \"StockLlama/ETH-USD-2022-01-01_2024-08-30\" 데이터셋으로, 2022년 1월 1일부터 2024년 8월 30일까지의 이더리움(ETH)과 미국 달러(USD) 환율 데이터를 포함하고 있습니다.\n",
        "\n",
        "이 데이터셋은 train이라는 단일 데이터 스플릿으로 제공되며, 종가, 최고가, 최저가, 장마감 등의 주식에서 볼 수 있는 값들을 column으로 가지고 있으며, 주요 task는 다음 시점의 값을 예측하는 것입니다.\n",
        "\n",
        "### **데이터셋 활용 방식**\n",
        "- 데이터는 총 716개로 구성되어 있습니다.\n",
        "- train : test 데이터가 따로 나누어져 있지 않으며, 임의로 0.8 : 0.1 : 0.1 비율로 구성하여 사용하였습니다.\n",
        "- 시간 정보는 따로 제공되지 않으며, 단순히 값이 나열되어 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "## **결과에 대한 고찰**\n",
        "\n",
        "모델에 상관없이, 다행히도 epoch이 늘어남에 따라 train loss와 test loss가 잘 감소하는 것을 볼 수 있었습니다. 그러나 Test Loss의 경우, 예상과 달라 상당히 놀랐습니다. 예상은 LSTM → GRU → RNN 순으로 test loss가 나올 것이라고 생각했으나, 실제 결과는 그렇지 않았습니다. 그 이유에 대해 분석해보고자 합니다.\n",
        "\n",
        "### **TEST LOSS 결과**\n",
        "- **RNN**: 104.2288\n",
        "- **LSTM**: 91.5012\n",
        "- **GRU**: 107.9035\n",
        "\n",
        "### **분석**\n",
        "1. **Dataset이 작아 long-term dependency가 발생하지 않았다.**\n",
        "   - 데이터셋은 총 716개로, 그 중에서도 50개를 보고 51번째 값을 예측하도록 설정했습니다. 이 길이가 너무 짧아 GRU와 LSTM의 구조가 RNN에 비해 그리 효과적이지 않았을 수 있습니다.\n",
        "\n",
        "2. **Dataset이 일관적인 경향성을 띄고 있다.**\n",
        "   - 만약 데이터가 시계열 데이터가 아닌 문자열 데이터였다면, 그 관계성이 비교적 모호해 학습이 덜 되었을 수 있습니다. 하지만 ETH 값에 대한 데이터는 비교적 예측이 가능했던 것으로 보입니다.\n",
        "\n",
        "3. **Epoch 수가 부족했다.**\n",
        "   - Loss가 떨어지는 모습을 보면 초기보다 감소해 local minima에 빠진 것처럼 보이지만, 각 값이 여전히 떨어지고 있다는 점에서 더 많은 epoch을 통해 학습이 진행되면, 더 극명한 성능 차이를 알 수 있을 것 같습니다.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "X14vjeTXZGv9"
      }
    }
  ]
}