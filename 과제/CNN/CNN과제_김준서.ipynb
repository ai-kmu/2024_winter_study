{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kp96wgcI-rdM"
      },
      "outputs": [],
      "source": [
        "# ./data 디렉토리 생성\n",
        "!mkdir -p ./data\n",
        "\n",
        "# images.tar.gz 압축 해제\n",
        "!tar -xzf images.tar.gz -C ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. !mkdir -p ./data\n",
        "\n",
        "\n",
        "    - mkdir: 디렉토리를 생성하는 명령어\n",
        "\t- -p: 이 옵션은 부모 디렉토리도 함께 생성하도록 해주는 옵션  \n",
        "\n",
        "    예를 들어, ./data 디렉토리가 이미 존재하면 오류를 발생시키지 않고 디렉토리가 없으면 생성\n",
        "\t- ./data: 생성할 디렉토리의 경로  \n",
        "     현재 작업 디렉토리(./)에 data라는 이름의 디렉토리를 생성하겠다는 의미\n",
        "\n",
        "2. !tar -xzf images.tar.gz -C ./data\n",
        "\n",
        "\n",
        "    - tar: 파일을 압축하거나 압축을 해제하는 명령어\n",
        "\t- -x: 압축을 해제하는 옵션\n",
        "\t- -z: .gz 확장자를 가진 gzip 형식의 압축 파일을 다루겠다는 옵션\n",
        "\t- -f: 파일 이름을 지정하는 옵션  \n",
        "    그 뒤에 오는 images.tar.gz는 압축 해제할 파일 이름\n",
        "\t- -C ./data: 압축 해제된 파일을 ./data 디렉토리로 이동시키겠다는 옵션 즉, 현재 작업 디렉토리의 data 폴더에 압축된 파일들을 풀어 놓겠다는 의미\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B8tqwYHCTo4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4UkcWhz-s9z",
        "outputId": "bed568de-3648-4633-902f-be808eb1455a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abyssinian_100.jpg\n",
            "Abyssinian_100.mat\n",
            "Abyssinian_101.jpg\n",
            "Abyssinian_101.mat\n",
            "Abyssinian_102.jpg\n",
            "Abyssinian_102.mat\n",
            "Abyssinian_103.jpg\n",
            "Abyssinian_104.jpg\n",
            "Abyssinian_105.jpg\n",
            "Abyssinian_106.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls ./data/images | head # 디렉토리 내의 파일 목록 나열, 첫 10개 항목 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 클래스\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RFPN6JlzUle4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu6vS42k_96L",
        "outputId": "1dfcb37a-db2d-4bf2-d365-a6b4871d7f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Size: 2944\n",
            "Validation Dataset Size: 736\n",
            "Test Dataset Size: 3669\n",
            "First Train Image Shape: torch.Size([3, 128, 128]), Label: 31\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# PetDataset 클래스 정의\n",
        "class PetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Oxford-IIIT Pet Dataset용 커스텀 Dataset 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, annotation_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): 데이터셋의 루트 디렉토리 경로 (예: './data').\n",
        "            annotation_file (str): 어노테이션 파일 경로 (예: './data/trainval.txt').\n",
        "            transform (callable, optional): 이미지에 적용할 변환(transform) 함수.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # 이미지 디렉토리와 라벨 매칭\n",
        "        image_dir = os.path.join(root_dir, 'images')\n",
        "        image_files = set(os.listdir(image_dir))\n",
        "\n",
        "        # 어노테이션 파일 읽기\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # 이미지 이름과 라벨 매칭\n",
        "        self.data = []\n",
        "        for line in lines:\n",
        "            components = line.strip().split()\n",
        "            image_name = components[0] + '.jpg'\n",
        "            label = int(components[1]) - 1  # 라벨을 0-based로 변환\n",
        "            if image_name in image_files:\n",
        "                self.data.append((image_name, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): 데이터 인덱스\n",
        "\n",
        "        Returns:\n",
        "            image (Tensor): 전처리된 이미지\n",
        "            label (int): 이미지의 클래스 라벨\n",
        "        \"\"\"\n",
        "        image_name, label = self.data[idx]\n",
        "        image_path = os.path.join(self.root_dir, 'images', image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transform 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # 이미지 크기 조정\n",
        "    transforms.ToTensor(),         # Tensor 변환\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화\n",
        "])\n",
        "\n",
        "# 데이터셋 경로 및 어노테이션 파일 경로\n",
        "root_dir = './data'\n",
        "trainval_annotation_file = 'trainval.txt'\n",
        "test_annotation_file = 'test.txt'\n",
        "\n",
        "# Train/Validation/Test Dataset 생성\n",
        "trainval_dataset = PetDataset(root_dir, trainval_annotation_file, transform=transform)\n",
        "test_dataset = PetDataset(root_dir, test_annotation_file, transform=transform)\n",
        "\n",
        "# Train/Validation Split\n",
        "train_size = int(0.8 * len(trainval_dataset))\n",
        "val_size = len(trainval_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(trainval_dataset, [train_size, val_size])\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# 데이터셋 크기 확인\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Validation Dataset Size: {len(val_dataset)}\")\n",
        "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
        "\n",
        "# 첫 번째 데이터 확인\n",
        "image, label = train_dataset[0]\n",
        "print(f\"First Train Image Shape: {image.shape}, Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oxford-IIIT Pet Dataset을 사용하여 PyTorch에서 학습을 위한 데이터셋을 준비, 해당 데이터셋을 DataLoader로 로드하여 훈련 및 검증을 수행할 수 있게 만드는 과정\n",
        "\n",
        "1. PetDataset 클래스 (Custom Dataset 클래스)\n",
        "\n",
        "\n",
        "    - PetDataset 클래스는 torch.utils.data.Dataset을 상속받아 사용자 정의 데이터셋 클래스를 만듭니다. 이 클래스는 이미지 파일과 해당하는 라벨을 매칭하여 반환합니다.\n",
        "\n",
        "__init__ 메서드:\n",
        "\n",
        "    - root_dir: 데이터셋의 루트 디렉토리 경로\n",
        "\t- annotation_file: 어노테이션 파일의 경로로, trainval.txt와 test.txt 파일을 사용\n",
        "\t- transform: 이미지에 적용할 변환 함수입니다. 예를 들어, 크기 조정, 텐서 변환, 정규화\n",
        "\t- image_dir: 이미지 파일이 저장된 디렉토리입 (root_dir/images)\n",
        "\t- image_files: 이미지 파일들의 집합을 만들어 모든 이미지를 읽을 수 있도록 함\n",
        "\t- 어노테이션 파일(trainval.txt, test.txt)을 읽어들여 각 이미지와 그에 해당하는 라벨을 매칭\n",
        "\t- self.data: 각 이미지의 경로와 라벨을 튜플 형태로 저장\n",
        "\n",
        "__len__ 메서드:\n",
        "\n",
        "    - 데이터셋의 크기를 반환 즉, 데이터셋에 포함된 이미지의 개수를 반환\n",
        "\n",
        "__getitem__ 메서드:\n",
        "\n",
        "    - 주어진 인덱스 idx에 대해 이미지와 라벨을 반환\n",
        "\t- 이미지는 PIL 라이브러리로 로드되며, transform이 주어지면 해당 변환을 적용\n",
        "\n",
        "2. Transform 정의\n",
        "\n",
        "\n",
        "\n",
        "    - transform은 torchvision.transforms의 기능을 사용하여 이미지에 적용할 전처리 과정을 정의\n",
        "\t- transforms.Resize((128, 128)): 이미지를 128x128 크기로 리사이즈\n",
        "\t- transforms.ToTensor(): 이미지를 텐서로 변환\n",
        "\t- transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]): 이미지를 정규화\n",
        "\n",
        "3. 데이터셋 경로 및 어노테이션 파일 경로 설정\n",
        "\n",
        "\n",
        "\n",
        "    - root_dir: 데이터셋이 저장된 루트 디렉토리 이 경로에 이미지와 어노테이션 파일이 존재\n",
        "\t- trainval_annotation_file 및 test_annotation_file: 각각 훈련용/검증용 및 테스트용 이미지에 대한 어노테이션 파일\n",
        "\n",
        "4. Train/Validation Split\n",
        "\n",
        "\n",
        "\t- trainval_dataset: 훈련 및 검증 데이터셋을 포함하는 객체\n",
        "\t- train_size와 val_size: 훈련용 데이터와 검증용 데이터의 비율을 80%와 20%로 나누어 훈련 데이터셋과 검증 데이터셋을 분할\n",
        "\t- random_split 함수를 사용하여 훈련과 검증 데이터셋을 랜덤하게 분리\n",
        "\n",
        "5. DataLoader 생성\n",
        "\n",
        "\n",
        "\t- train_loader, val_loader, test_loader: DataLoader 객체는 배치 처리를 지원하며 데이터를 효율적으로 불러오기 위해 사용\n",
        "\t- train_loader: 훈련 데이터셋을 배치 크기 32로 불러옴\n",
        "\t- val_loader: 검증 데이터셋을 배치 크기 32로 불러옴\n",
        "\t- test_loader: 테스트 데이터셋을 배치 크기 32로 불러옴\n",
        "\t- num_workers=2: 데이터를 로드할 때 사용할 프로세스 수\n",
        "\n",
        "6. 데이터셋 크기 및 샘플 확인\n",
        "\n",
        "\n",
        "\n",
        "\t- print(f\"Train Dataset Size: {len(train_dataset)}\"): 훈련 데이터셋의 크기를 출력\n",
        "\t- print(f\"Validation Dataset Size: {len(val_dataset)}\"): 검증 데이터셋의 크기를 출력\n",
        "\t- print(f\"Test Dataset Size: {len(test_dataset)}\"): 테스트 데이터셋의 크기를 출력\n",
        "\t- image, label = train_dataset[0]: 훈련 데이터셋에서 첫 번째 이미지를 가져옴\n",
        "\t- print(f\"First Train Image Shape: {image.shape}, Label: {label}\"): 첫 번째 이미지의 형태와 해당 라벨을 출력\n",
        "\n"
      ],
      "metadata": {
        "id": "uGLqYa2dU3EZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lajbJzfMCZR_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        # Block 1: Convolutional Layer, BatchNorm, Dropout, Skip Connection\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # Skip connection for Block 1\n",
        "        self.skip_conv1 = nn.Conv2d(3, 64, kernel_size=1, stride=2, padding=0)  # Skip connection with downsampling\n",
        "        self.skip_bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Block 2: Convolutional Layer, BatchNorm, Dropout, Skip Connection\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "\n",
        "        # Skip connection for Block 2\n",
        "        self.skip_conv2 = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0)\n",
        "        self.skip_bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Block 3: Convolutional Layer, BatchNorm, Dropout\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc_input_dim = 256 * 16 * 16  # Assuming input image size is 128x128\n",
        "        self.fc1 = nn.Linear(self.fc_input_dim, 512)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1 with skip connection\n",
        "        residual = self.skip_bn1(self.skip_conv1(x))  # Downsampled residual\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x += residual  # Skip connection after pooling\n",
        "\n",
        "        # Block 2 with skip connection\n",
        "        residual = self.skip_bn2(self.skip_conv2(x))  # Downsampled residual\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x += residual  # Skip connection after pooling\n",
        "\n",
        "        # Block 3 without skip connection\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. __init__ 메서드 (모델 초기화)\n",
        "\n",
        "모델의 레이어를 정의하는 부분  \n",
        "각 레이어는 nn.Conv2d, nn.BatchNorm2d, nn.Dropout 등을 사용하여 초기화\n",
        "\n",
        "\n",
        "\t- conv1, conv2: 첫 번째 합성곱 층 conv1은 입력 채널 수인 3(색상 채널)을 64개의 특징 맵으로 변환하며, conv2는 conv1의 출력인 64개의 채널을 다시 64개로 변환\n",
        "\t- bn1, bn2: 각각 conv1, conv2 뒤에 배치 정규화를 적용하여, 학습을 안정시키고 빠르게 수렴하도록 도움\n",
        "\t- pool1: MaxPool2d를 사용해 2x2 풀링을 하여 공간적인 크기를 절반으로 줄임\n",
        "\t- dropout1: 드롭아웃을 30%로 적용하여 과적합을 방지\n",
        "\n",
        "Skip Connection:\n",
        "\n",
        "\t- skip_conv1, skip_bn1: conv1 레이어와 conv2 레이어의 입력을 그대로 전달하려는 목적의 스킵 연결 이 연결은 합성곱 연산 후 차원을 맞추기 위해 1x1 커널을 사용하고, 배치 정규화가 적용됨\n",
        "\t- conv3, conv4: 두 번째 합성곱 층 conv3은 64개의 채널을 128개의 특징 맵으로 변환하고, conv4는 conv3의 출력인 128개의 채널을 다시 128개로 변환\n",
        "\t- bn3, bn4: 각각 conv3, conv4 뒤에 배치 정규화를 적용\n",
        "\t- pool2: MaxPool2d로 풀링을 하여 공간 크기를 절반으로 줄임\n",
        "\t- dropout2: 드롭아웃을 40%로 설정하여 과적합을 방지\n",
        "\n",
        "\n",
        "Skip Connection:\n",
        "\n",
        "\t- skip_conv2, skip_bn2: conv3와 conv4의 출력을 스킵 연결로 전달하기 위한 레이어\n",
        "\t- conv5, conv6: 세 번째 합성곱 층 conv5는 128개의 채널을 256개의 특징 맵으로 변환하고, conv6은 conv5의 출력인 256개의 채널을 다시 256개로 변환\n",
        "\t- bn5, bn6: 각각 conv5, conv6 뒤에 배치 정규화를 적용\n",
        "\t- pool3: 다시 한 번 풀링을 하여 출력 크기를 절반으로 줄임\n",
        "\t- dropout3: 드롭아웃을 50%로 설정하여 과적합을 방지\n",
        "\t- Fully Connected Layer (FC):\n",
        "\t- fc_input_dim: 256 * 16 * 16은 입력 이미지 크기가 128x128일 때, 합성곱 연산 후에 얻어지는 특징 맵의 차원 이를 1D 벡터로 평탄화하여 FC 레이어에 전달함\n",
        "\t- fc1: 첫 번째 완전 연결(fully connected) 층으로 512개의 출력을 가짐\n",
        "\t- dropout4: FC 층에도 드롭아웃을 적용하여 과적합을 방지\n",
        "\t- fc2: 마지막 출력층으로, num_classes개의 출력을 가짐 이는 분류할 클래스의 개수\n",
        "\n",
        "\n",
        "2. forward 메서드 (순전파)\n",
        "\n",
        "forward 메서드는 모델에 대한 순전파 과정을 정의  \n",
        "입력 데이터가 모델을 통해 어떻게 처리되는지 설명\n",
        "\n",
        "    - 첫 번째  블록 (Block 1):\n",
        "\t    - residual: skip_conv1과 skip_bn1을 사용하여 스킵 연결로 전달할 값을 계산\n",
        "\t    - x = F.relu(self.bn1(self.conv1(x))): conv1에 배치 정규화와 ReLU 활성화 함수를 적용\n",
        "\t    - x += residual: 이전에 계산된 residual을 더하여 스킵 연결을 구현\n",
        "\n",
        "\t- 두 번째 블록 (Block 2):\n",
        "\t    - residual: skip_conv2와 skip_bn2를 사용하여 스킵 연결을 전달할 값을 계산\n",
        "\t    - x = F.relu(self.bn3(self.conv3(x))): conv3에 배치 정규화와 ReLU를\n",
        "        적용\n",
        "\t    - x += residual: 이전에 계산된 residual을 더하여 스킵 연결을 구현\n",
        "\n",
        "\t- 세 번째 블록 (Block 3):\n",
        "\t    - x = F.relu(self.bn5(self.conv5(x))): conv5에 배치 정규화와 ReLU를 적용 이후 풀링과 드롭아웃을 적용\n",
        "\n",
        "\t- 평탄화 (Flatten):\n",
        "\t    - x.view(x.size(0), -1): 3D 텐서를 1D로 평탄화하여 FC 층에 전달할 수 있게 함\n",
        "\n",
        "\t- 완전 연결 층 (Fully Connected Layers):\n",
        "\t    - x = F.relu(self.fc1(x)): 첫 번째 FC 층을 통과시킴\n",
        "\t    - x = self.fc2(x): 마지막 FC 층을 통과시켜 최종 출력을 얻음"
      ],
      "metadata": {
        "id": "JRCi37MsYVyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxWfdLVRCxKD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "num_classes = 37\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001  # AdamW에서 잘 동작하는 학습률로 설정\n",
        "weight_decay = 1e-4  # L2 규제\n",
        "\n",
        "# 디바이스 설정 (GPU 또는 CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델, 손실 함수, 옵티마이저 설정\n",
        "model = CustomCNN(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# CosineAnnealingWarmRestarts 스케줄러\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# 학습과 검증 결과 기록\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Learning rate 조정 (CosineAnnealing)\n",
        "    scheduler.step()\n",
        "\n",
        "    # 학습률 출력 (현재 학습률을 출력)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    # 출력 (매 에폭마다 train loss, val loss, train accuracy, val accuracy 출력)\n",
        "    clear_output(wait=True)  # 이전 출력 삭제\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "          f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # 실시간 그래프 출력 (Loss, Accuracy)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epoch + 2), train_losses, label='Train Loss', marker='o')\n",
        "    plt.plot(range(1, epoch + 2), val_losses, label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epoch + 2), train_accuracies, label='Train Accuracy', marker='o')\n",
        "    plt.plot(range(1, epoch + 2), val_accuracies, label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Train and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 하이퍼파라미터 설정\n",
        "\n",
        "\n",
        "    - num_classes: 데이터셋에서의 클래스 수 (여기서는 37).\n",
        "\t- num_epochs: 모델 학습을 반복할 횟수 여기서는 80 에폭으로 설정\n",
        "\t- learning_rate: 모델 학습의 초기 학습률을 설정 AdamW 옵티마이저에서 잘 동작하는 값으로 설정\n",
        "\t- weight_decay: L2 규제값으로, 모델의 가중치가 과도하게 커지지 않도록 돕는 역할\n",
        "\n",
        "2. 디바이스 설정\n",
        "\n",
        "\n",
        "\t- device: 모델이 실행될 디바이스를 설정 GPU가 사용 가능한 경우 GPU를, 그렇지 않으면 CPU를 사용\n",
        "\n",
        "3. 모델, 손실 함수,옵티마이저 설정\n",
        "\n",
        "\n",
        "\t- model: CustomCNN이라는 커스텀 CNN 모델을 정의하여 device에 맞게 전송\n",
        "\t- criterion: 손실 함수로 CrossEntropyLoss를 사용 다중 클래스 분류 문제에서 자주 사용하는 손실 함수\n",
        "\t- optimizer: AdamW 옵티마이저를 사용하며, 학습률과 L2 규제를 설정\n",
        "\n",
        "4. CosineAnnealingWarmRestarts 스케줄러 설정\n",
        "\n",
        "\n",
        "\t- scheduler: CosineAnnealingWarmRestarts는 학습률을 일정 주기로 코사인 곡선처럼 줄이고, 다시 주기적으로 학습률을 증가시키는 스케줄러\n",
        "\t- T_0=10: 첫 번째 주기의 길이를 10으로 설정\n",
        "\t- T_mult=2: 주기가 끝날 때마다 주기의 길이를 2배로 늘림\n",
        "\t- eta_min=1e-6: 최소 학습률을 1e-6으로 설정\n",
        "\n",
        "5. 학습 및 검증 기록\n",
        "\n",
        "\n",
        "\t- 학습 중에 기록할 변수들을 초기화\n",
        "    - 각 에폭마다 train_losses, val_losses, train_accuracies, val_accuracies 리스트에 값을 추가하여 후에 결과를 분석\n",
        "\n",
        "6. 학습 루프  \n",
        "\n",
        "\n",
        "\t- 학습 모드로 설정: model.train()은 모델을 학습 모드로 설정하여 Dropout이나 BatchNorm과 같은 레이어가 학습 중처럼 동작하도록 함\n",
        "\t- 배치마다 연산: train_loader에서 배치 단위로 이미지를 불러와서, 모델에 입력하고 손실을 계산하여, 역전파(backpropagation) 후 옵티마이저로 업데이트\n",
        "\t- 손실과 정확도 기록: 각 배치의 손실과 정확도를 계산하여 running_loss, correct, total에 누적\n",
        "\n",
        "7. 검증 루프\n",
        "\n",
        "\n",
        "\t- 평가 모드 설정: model.eval()은 모델을 평가 모드로 설정 이 모드에서는 Dropout과 BatchNorm이 평가 모드로 동작\n",
        "\t- 검증 배치 처리: torch.no_grad()는 gradient 계산을 하지 않도록 하여 테스트에 소모되는 메모리와 시간을 줄임\n",
        "    - 검증 데이터에 대해 예측을 하고 손실 및 정확도를 계산\n",
        "\n",
        "8. 학습률 스케줄링\n",
        "\n",
        "\n",
        "\t- 학습률 스케줄러가 각 에폭마다 학습률을 조정하도록 함\n",
        "    - CosineAnnealingWarmRestarts는 학습률을 일정 주기로 조정하여 모델이 더 잘 수렴할 수 있도록 도움\n",
        "\n",
        "9. 실시간 그래프 출력\n",
        "\n",
        "\n",
        "\t- Loss: train_losses와 val_losses를 시각화하여 훈련과 검증의 손실을 비교\n",
        "\t- Accuracy: train_accuracies와 val_accuracies를 시각화하여 훈련과 검증 정확도를 비교"
      ],
      "metadata": {
        "id": "lxIfpC54cBnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 전환\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # 테스트 시에는 gradient 계산을 하지 않음\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()  # 손실값 누적\n",
        "\n",
        "            # 예측 결과 계산\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # 평균 테스트 손실 및 정확도 계산\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100 * correct / total\n",
        "\n",
        "    # 테스트 결과 출력\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "dFF8KHiEgUl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_model() 함수는 주어진 테스트 데이터셋에 대해 모델을 평가하는 함수  \n",
        "이 함수는 모델을 평가 모드로 전환하고, 테스트 데이터셋을 사용하여 손실(loss)과 정확도(accuracy)를 계산\n",
        "\n",
        "\n",
        "1.\tmodel.eval():\n",
        "\n",
        "\n",
        "\t- 이 명령어는 모델을 “평가 모드”로 전환\n",
        "    - 평가 모드에서는 Dropout 레이어나 Batch Normalization 레이어가 학습 모드와 다르게 동작 - 학습 시에는 랜덤하게 뉴럴 네트워크를 비활성화하는 Dropout과 같은 레이어가 평가 시에는 항상 활성화됨\n",
        "\n",
        "2.\ttest_loss = 0.0, correct = 0, total = 0:\n",
        "\n",
        "\n",
        "\t- 테스트 과정에서 사용할 변수들을 초기화\n",
        "\t- test_loss: 테스트 데이터셋의 평균 손실을 계산하기 위한 변수\n",
        "\t- correct: 정확도를 계산하기 위해 맞춘 예측의 수\n",
        "\t- total: 전체 샘플 수\n",
        "\n",
        "3.\twith torch.no_grad()::\n",
        "\n",
        "\n",
        "\t- torch.no_grad()는 그라디언트 계산을 비활성화 이는 테스트 시에는 역전파(backpropagation) 과정이 필요 없기 때문에 메모리 사용과 계산 속도를 줄여줌\n",
        "    - 이 블록 안에서는 파라미터의 업데이트가 이루어지지 않음\n",
        "\n",
        "4.\tfor images, labels in test_loader::\n",
        "\n",
        "\n",
        "\t- test_loader는 테스트 데이터를 배치(batch) 단위로 불러오는 DataLoader  \n",
        "    - 각 배치마다 images와 labels가 test_loader에서 반환\n",
        "\n",
        "5.\timages, labels = images.to(device), labels.to(device):\n",
        "\n",
        "\n",
        "\t- images와 labels를 설정된 device (GPU 또는 CPU)로 이동시킴\n",
        "    - GPU에서 모델을 학습시키고 있다면 데이터를 GPU로 옮겨서 처리 속도를 빠르게 함\n",
        "\n",
        "6.\toutputs = model(images):\n",
        "\n",
        "\n",
        "\t- 모델에 이미지를 입력하여 예측된 값을 outputs로 받음\n",
        "\n",
        "7.\tloss = criterion(outputs, labels):\n",
        "\n",
        "\n",
        "\t- 모델의 예측값(outputs)과 실제 레이블(labels)을 비교하여 손실을 계산\n",
        "    - criterion은 손실 함수로, 보통 CrossEntropyLoss 같은 분류 손실 함수를 사용\n",
        "\n",
        "8.\ttest_loss += loss.item():\n",
        "\n",
        "\n",
        "\t- 배치마다 계산된 손실값을 test_loss에 누적\n",
        "    - .item()은 텐서에서 값을 추출하여 파이썬 숫자 형태로 반환\n",
        "\n",
        "9.\t_, predicted = torch.max(outputs.data, 1):\n",
        "\n",
        "\n",
        "\t- outputs.data는 모델의 예측 결과 torch.max() 함수는 각 배치에 대해 가장 큰 값을 갖는 인덱스를 반환 predicted는 모델이 예측한 클래스 인덱스를 나타냄\n",
        "\n",
        "10.\ttotal += labels.size(0):\n",
        "\n",
        "\n",
        "\t- 현재 배치에서의 샘플 수를 total에 추가\n",
        "\n",
        "11.\tcorrect += (predicted == labels).sum().item():\n",
        "\n",
        "\n",
        "\t- 예측한 클래스(predicted)와 실제 레이블(labels)이 일치하는지를 확인하여 맞춘 예측의 개수를 누적\n",
        "\n",
        "12.\ttest_loss /= len(test_loader):\n",
        "\n",
        "\n",
        "\t- 전체 배치에 대해 평균 손실을 계산\n",
        "\n",
        "13.\ttest_accuracy = 100 * correct / total:\n",
        "\n",
        "\n",
        "\t- 전체 테스트 데이터에서 모델의 정확도를 계산\n",
        "    - 정확도는 맞춘 예측의 수를 전체 샘플 수로 나누어 백분율로 계산\n",
        "\n",
        "14.\tprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\"):\n",
        "\n",
        "\n",
        "\t- 계산된 테스트 손실과 정확도를 출력\n",
        "    \n",
        "15. return test_loss, test_accuracy:\n",
        "\n",
        "\n",
        "\t- 최종 테스트 손실과 정확도를 반환"
      ],
      "metadata": {
        "id": "hWB-iAcGgiqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특이사항:   \n",
        "처음에 데이터셋을 다운받았는데 annotation안에서 어떤 파일을 이용해서 학습 및 테스트를 진행하는지 혼동이 와서 진행이 더딘 점이 있었다  \n",
        "학습 진행 중 GPU를 다 써서 학습 완료를 하지 못하였음\n",
        "\n",
        "### 분석, 고찰:   \n",
        "주석 및 설명\n",
        "\n",
        "### 어려웠던 점:\n",
        "모델 구축은 주로 로컬 컴퓨터에서 진행했었고, 코랩을 이용해본 건 거의 처음이었습니다. 로컬에서는 GPU나 연결 문제로 작업이 중단되는 일이 드물지만, 코랩 환경에서는 불편함을 겪었습니다. 모델 구조를 설정하고 코드를 작성하는 과정에서 차원 오류가 많이 발생했고, 이를 해결하는 데 어려움이 있었습니다. 각 레이어에서 계산된 결과를 다음 레이어로 넘길 때 발생하는 오류를 줄이고, 계산이 제대로 이루어지도록 하는 데 추가적인 공부가 필요하다고 느꼈습니다.\n",
        "\n",
        "하이퍼파라미터 튜닝을 진행했지만, 낮은 에폭수에서는 큰 체감을 얻지 못했으며, 학습을 완료하지 못해 모델 성능의 한계인지, 아니면 튜닝이 부족한 것인지를 명확히 파악하기 어려웠습니다. 모델에 따라, 데이터셋에 따라 하이퍼파라미터(학습률, 옵티마이저, 배치 사이즈 등)의 설정이 천차만별이라, 효율적으로 원하는 결과를 빠르게 도출해내는 것이 어려웠습니다. 이를 해결하기 위해서는 경험이 중요하다고 느꼈습니다. 각 하이퍼파라미터들이 최적화되기 위해서는 어떤 설정이 효과적인지, 일반적으로 사용되는 초기 학습 설정에 대해 더 공부할 필요가 있다고 생각합니다.\n",
        "\n",
        "WandB(Weights & Biases)와 같은 하이퍼파라미터 최적화, 실험 추적, 모델 학습 과정 모니터링, 데이터셋 및 결과 시각화 도구를 사용하여 실험을 자동화하고 더 빠르게 원하는 결과를 확인하는 방법이 현재로썬 가장 효율적일 것 같다고 느꼈습니다."
      ],
      "metadata": {
        "id": "wbAmnVx3cBez"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}