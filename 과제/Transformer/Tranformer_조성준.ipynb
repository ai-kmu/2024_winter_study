{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0. 필요한 라이브러리 import**\n",
        "\n",
        "hugging face 의 모델들을 사용하기 위해서 아래와 같은 모듈들을 import 해준다.\n",
        "\n",
        "**transformers** : Hugging Face에서 제공하는 라이브러리로, 다양한 사전 학습된 트랜스포머 모델(GPT, BERT, T5 등)을 쉽게 불러오고 파인튜닝할 수 있도록 지원한다.\n",
        "주요 기능: 모델 로드, 토크나이저, 학습, 추론\n",
        "\n",
        "**datasets** : Hugging Face에서 제공하는 대규모 데이터셋 라이브러리로.\n",
        "datasets.load_dataset()을 이용해서 다양한 공개 데이터셋(KorQuAD, SQuAD, IMDB 등)을 쉽게 불러올 수 있으며.\n",
        "데이터 로딩, 전처리, 변환, 배치 단위 처리 등을 지원한다.\n",
        "\n",
        "**wandb** : 머신러닝 실험을 추적, 로깅하고 시각화할 수 있는 도구로\n",
        "학습 과정에서 손실(loss), 정확도(accuracy) 등을 저장하고 대시보드에서 확인할 수 있다.\n",
        "wandb.init()을 사용해서 프로젝트를 생성하고 학습 로그를 관리할 수 있다."
      ],
      "metadata": {
        "id": "twNTyVPvPFqb"
      },
      "id": "twNTyVPvPFqb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568a544d-3d62-49a3-b033-d388fe9380ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T15:58:58.765387Z",
          "iopub.status.busy": "2025-02-23T15:58:58.764734Z",
          "iopub.status.idle": "2025-02-23T15:59:04.817307Z",
          "shell.execute_reply": "2025-02-23T15:59:04.815449Z",
          "shell.execute_reply.started": "2025-02-23T15:58:58.765328Z"
        },
        "id": "568a544d-3d62-49a3-b033-d388fe9380ea",
        "outputId": "fce1ef97-be89-44cd-f6b3-db3599bf2b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: http://repo.ai.gato/registry/repository/pypi-proxy/simple\n",
            "Requirement already satisfied: transformers in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (4.49.0)\n",
            "Requirement already satisfied: datasets in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (3.3.2)\n",
            "Requirement already satisfied: wandb in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (0.19.7)\n",
            "Requirement already satisfied: filelock in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/fsspec/2024.12.0/fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb) (70.3.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Installing collected packages: fsspec\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.12.0 requires numba-cuda<0.0.18,>=0.0.13, which is not installed.\n",
            "s3fs 2025.2.0 requires fsspec==2025.2.0.*, but you have fsspec 2024.12.0 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 데이터 전처리**\n",
        "\n",
        "KorQuAD 데이터셋을 로드하고 KLUE-BERT를 사용하여 토큰화한 후, 훈련을 위한 전처리를 수행하는 과정이다.  \n",
        "\n",
        "### **전처리 과정**\n",
        "1. **context + question을 BERT 토크나이저로 토큰화** (**512 토큰 제한**)  \n",
        "2. **정답(Answer)의 문자 위치 → 토큰 위치로 변환**  \n",
        "3. **start_positions, end_positions를 추가하여 모델이 훈련할 수 있도록 변환**  \n",
        "\n",
        "> 나는 **Encoder 기반의 BERT 모델**을 사용하기 때문에, `answer`를 생성하는 **생성형(Generative) 모델**들과는 달리, 문장의 **시작, 끝 위치를 예측하는 방식**으로 모델을 구성했다.\n"
      ],
      "metadata": {
        "id": "7Hqsfl7eQR55"
      },
      "id": "7Hqsfl7eQR55"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de760e6-ab05-4e3d-aeae-e6d5bcf177d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T15:59:04.820854Z",
          "iopub.status.busy": "2025-02-23T15:59:04.819867Z",
          "iopub.status.idle": "2025-02-23T15:59:25.477379Z",
          "shell.execute_reply": "2025-02-23T15:59:25.476217Z",
          "shell.execute_reply.started": "2025-02-23T15:59:04.820775Z"
        },
        "id": "5de760e6-ab05-4e3d-aeae-e6d5bcf177d9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset(\"KorQuAD/squad_kor_v1\")\n",
        "\n",
        "# 사용할 사전 학습 모델 선택 (KLUE BERT 사용)\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 전처리 함수 정의\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples[\"context\"],\n",
        "        examples[\"question\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    # 정답(Answer)의 시작 위치 찾기\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, answer in enumerate(examples[\"answers\"]):\n",
        "        if len(answer[\"text\"]) == 0:  # 정답이 없는 경우\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            start_char = answer[\"answer_start\"][0]  # 정답의 시작 위치 (문자 기준)\n",
        "            end_char = start_char + len(answer[\"text\"][0])  # 정답의 끝 위치\n",
        "\n",
        "            # 정답이 `input_ids` 내 어느 토큰에 해당하는지 찾기\n",
        "            token_start_index = inputs.char_to_token(i, start_char)\n",
        "            token_end_index = inputs.char_to_token(i, end_char - 1)\n",
        "\n",
        "            if token_start_index is None or token_end_index is None:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                start_positions.append(token_start_index)\n",
        "                end_positions.append(token_end_index)\n",
        "\n",
        "    # 최종 데이터셋 반환\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# 데이터셋 전처리 적용\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "# 데이터 형식 변환 (Trainer가 사용할 수 있도록 설정)\n",
        "tokenized_datasets.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 훈련 및 평가**\n",
        "\n",
        "이 코드는 Trainer를 사용하여 KLUE-BERT를 KorQuAD 데이터셋에 파인튜닝하는 과정으로 구성을 보면 크게 3단계로 나뉜다.\n",
        "1. 질문-답변(Task)에 특화된 BERT 모델을 자동으로 로드한다.\n",
        "2. trainingArguments를 사용해 훈련 설정 (학습률, 배치 크기, 로깅 등)\n",
        "3. Trainer를 사용해 모델, 데이터셋, 토크나이저 설정 후 학습 준비 완료\n",
        "\n",
        "**HuggingFace** 가 제공하는 아주 간편한 메서드를 통해 모델을 쉽게 불러올 수 있었는데, AutoModelForQuestionAnswering 을 통해 모델을 불러오게 되면, 모델 구조가 자동으로 QA task를 위해 맞춰지며, 기존 BERT 모델에 HEAD를 붙히는 식으로 구현된다. 즉 BACKBORN 을 유지하면서, 출력을 낼 수 있는 HEAD를 자동으로 붙히는"
      ],
      "metadata": {
        "id": "mRhwmvtzSq7R"
      },
      "id": "mRhwmvtzSq7R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1509868b-18da-40a3-80bc-1e4bec87fb45",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T15:59:25.479628Z",
          "iopub.status.busy": "2025-02-23T15:59:25.479167Z",
          "iopub.status.idle": "2025-02-23T15:59:32.104500Z",
          "shell.execute_reply": "2025-02-23T15:59:32.103749Z",
          "shell.execute_reply.started": "2025-02-23T15:59:25.479602Z"
        },
        "id": "1509868b-18da-40a3-80bc-1e4bec87fb45",
        "outputId": "375f7991-d38c-4307-c195-17dc286c975e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_7203/608393248.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "# 모델 불러오기\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# 훈련 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,  # 최근 2개 모델만 저장\n",
        "    load_best_model_at_end=True,  # 가장 좋은 모델 저장\n",
        "    report_to=\"wandb\"  # W&B 로깅\n",
        ")\n",
        "\n",
        "# Trainer 정의\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer 클래스는 Hugging Face에서 제공하는 자동 학습 관리 도구로, 앞서 trainer를 만들 때 설정했던 설정 값들을 토대로 trainer 클래스가 학습하게 된다."
      ],
      "metadata": {
        "id": "NoF1kuCLVLR2"
      },
      "id": "NoF1kuCLVLR2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57d5fd5-7d35-435a-8582-7042962f772d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T15:59:32.105910Z",
          "iopub.status.busy": "2025-02-23T15:59:32.105673Z",
          "iopub.status.idle": "2025-02-23T18:20:06.811402Z",
          "shell.execute_reply": "2025-02-23T18:20:06.810328Z",
          "shell.execute_reply.started": "2025-02-23T15:59:32.105887Z"
        },
        "id": "b57d5fd5-7d35-435a-8582-7042962f772d",
        "outputId": "0233809b-71ad-48a9-f5a0-9edc02261c0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkk121026\u001b[0m (\u001b[33mkkk121026-kookmin-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (8.9s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/20203143/wandb/run-20250224_005933-5icmv09l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kkk121026-kookmin-university/huggingface/runs/5icmv09l' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/kkk121026-kookmin-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kkk121026-kookmin-university/huggingface' target=\"_blank\">https://wandb.ai/kkk121026-kookmin-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kkk121026-kookmin-university/huggingface/runs/5icmv09l' target=\"_blank\">https://wandb.ai/kkk121026-kookmin-university/huggingface/runs/5icmv09l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37755' max='37755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37755/37755 2:20:20, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.487700</td>\n",
              "      <td>0.512494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.317100</td>\n",
              "      <td>0.554175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.207800</td>\n",
              "      <td>0.614121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.139400</td>\n",
              "      <td>0.799269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.077400</td>\n",
              "      <td>0.965911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37755, training_loss=0.2602638837723144, metrics={'train_runtime': 8434.4087, 'train_samples_per_second': 35.81, 'train_steps_per_second': 4.476, 'total_flos': 7.892076592075776e+16, 'train_loss': 0.2602638837723144, 'epoch': 5.0})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 학습 시작\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 학습 종료 및 평가**\n",
        "\n",
        "학습 이후 `trainer.evaluate()`를 통해 **validation set**에 대한 결과를 볼 수 있으며,  \n",
        "학습된 모델의 **가중치**와 **토크나이저**도 함께 저장하게 된다.  \n",
        "\n",
        "단순히 **loss 값이 낮다**고 해서 모델이 잘 작동한다고 단정할 수 없으므로,  \n",
        "실제로 모델이 **예측을 얼마나 잘하는지** 확인하기 위해 **임의의 데이터를 생성하여 추론을 수행**했다.  \n",
        "\n",
        "실험 결과, 아래 예시를 포함하여 대부분의 질문에 대해 **모델이 올바르게 추론하는 것을 확인할 수 있었다.** 🚀  \n",
        "\n",
        "---\n",
        "\n",
        "## **실험 결과 예시**\n",
        "\n",
        "### 🔹 **예제 1**\n",
        "- **Context:**  \n",
        "  > *허깅페이스는 자연어 처리(NLP) 및 머신러닝을 위한 플랫폼이다. 주요 제품으로는 트랜스포머 라이브러리가 있다.*\n",
        "- **Question:**  \n",
        "  > *허깅페이스의 주요 제품은 무엇인가?*\n",
        "- **Answer (Ground Truth):**  \n",
        "  > *트랜스포머 라이브러리*\n",
        "- **Predict (모델 예측):**  \n",
        "  > *트랜스포머 라이브러리*\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **예제 2**\n",
        "- **Context:**  \n",
        "  > *아이작 뉴턴(Isaac Newton)은 17세기 영국의 수학자이자 물리학자로, 미적분학과 고전역학의 기초를 다졌다. 그의 저서 《프린키피아(Philosophiæ Naturalis Principia Mathematica)》는 물리학 역사에서 가장 영향력 있는 책 중 하나로 평가받는다*\n",
        "- **Question:**  \n",
        "  > *아이작 뉴턴은 어떤 학문에서 중요한 기여를 했는가?*\n",
        "- **Answer (Ground Truth):**  \n",
        "  > *미적분학과 고전역학*\n",
        "- **Predict (모델 예측):**  \n",
        "  > *미적분학과 고전역학*\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **예제 2**\n",
        "- **Context:**  \n",
        "  > *대한민국의 수도는 서울이며, 1천만 명이 넘는 인구가 거주하는 대도시이다. 서울은 정치, 경제, 문화의 중심지로서 다양한 역사적 유산과 현대적인 건축물이 공존하는 곳이다*\n",
        "- **Question:**  \n",
        "  > *대한민국의 수도는 어디인가?*\n",
        "- **Answer (Ground Truth):**  \n",
        "  > *서울*\n",
        "- **Predict (모델 예측):**  \n",
        "  > *서울*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "✔ **추가 실험에서도 대부분 정확한 답변을 도출할 수 있었음!**  \n",
        "📌 **하지만 직관적으로 답이 주어지지 않는 일부 어려운 질문에서는 부정확한 답변을 생성하는 경우도 있었다.**\n"
      ],
      "metadata": {
        "id": "O7FIjiA9V0yD"
      },
      "id": "O7FIjiA9V0yD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d64a48a-11be-4140-82f5-8802b5bfbb8b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T18:20:06.812907Z",
          "iopub.status.busy": "2025-02-23T18:20:06.812643Z",
          "iopub.status.idle": "2025-02-23T18:20:47.275857Z",
          "shell.execute_reply": "2025-02-23T18:20:47.274734Z",
          "shell.execute_reply.started": "2025-02-23T18:20:06.812884Z"
        },
        "id": "1d64a48a-11be-4140-82f5-8802b5bfbb8b",
        "outputId": "8a1abc10-2cff-422a-aef0-fcc72852797a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='722' max='722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [722/722 00:40]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results: {'eval_loss': 0.5124937295913696, 'eval_runtime': 40.4461, 'eval_samples_per_second': 142.758, 'eval_steps_per_second': 17.851, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results: {eval_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a23549f-5a8b-46f4-9545-78c18e253d5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-23T18:20:47.278487Z",
          "iopub.status.busy": "2025-02-23T18:20:47.278239Z",
          "iopub.status.idle": "2025-02-23T18:21:18.869159Z",
          "shell.execute_reply": "2025-02-23T18:21:18.867096Z",
          "shell.execute_reply.started": "2025-02-23T18:20:47.278466Z"
        },
        "id": "7a23549f-5a8b-46f4-9545-78c18e253d5f",
        "outputId": "f4d7dbd2-c098-41b3-badb-2d970c2514bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved at ./saved_model\n"
          ]
        }
      ],
      "source": [
        "save_path = \"./saved_model\"\n",
        "trainer.save_model(save_path)  # 모델 가중치 저장\n",
        "tokenizer.save_pretrained(save_path)  # 토크나이저도 저장\n",
        "\n",
        "print(f\"Model saved at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d667e0-c8e1-4652-a3c7-7df6c91ce0c1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-24T00:28:16.285922Z",
          "iopub.status.busy": "2025-02-24T00:28:16.285210Z",
          "iopub.status.idle": "2025-02-24T00:28:16.330039Z",
          "shell.execute_reply": "2025-02-24T00:28:16.329383Z",
          "shell.execute_reply.started": "2025-02-24T00:28:16.285862Z"
        },
        "id": "35d667e0-c8e1-4652-a3c7-7df6c91ce0c1",
        "outputId": "c3ad7525-e47d-4934-ad22-0eb92fa564c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: 누가 기침소리를 내었는가?\n",
            "Expected Answer: 영의정\n",
            "Predicted Answer: 영의정\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "\n",
        "context = \"기침소리를 낸 것은 영의정이다.\"\n",
        "question = \"누가 기침소리를 내었는가?\"\n",
        "expected_answer = \"영의정\"  # 실제 정답\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(context, question, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "# 가장 높은 확률을 가진 start, end 인덱스 찾기\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits)\n",
        "\n",
        "predicted_answer = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1])\n",
        ")\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Expected Answer: {expected_answer}\")\n",
        "print(f\"Predicted Answer: {predicted_answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KorQuAD (Korean Question Answering Dataset) 소개\n",
        "\n",
        "KorQuAD(Korean Question Answering Dataset)는 한국어 기계 독해(Machine Reading Comprehension, MRC) 태스크를 위한 데이터셋으로, Hugging Face에서는 `\"KorQuAD/squad_kor_v1\"`라는 이름으로 제공되고 있다.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 1. KorQuAD란?\n",
        "KorQuAD는 한국어 질문-답변 태스크(QA, Question Answering)를 학습하고 평가하기 위해 구축된 데이터셋이다. 이 데이터셋은 영어의 대표적인 QA 데이터셋인 **SQuAD**(Stanford Question Answering Dataset)를 참고하여 만들어졌다.\n",
        "\n",
        "- **KorQuAD 1.0**: 한국어 Wikipedia 문서에서 추출된 문단을 기반으로 구성됨.\n",
        "- **KorQuAD 2.0**: 추가적인 노이즈 데이터를 포함하여 질문이 더 어려워진 버전이지만, Hugging Face에서는 **1.0 버전만 제공**된다.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 2. \"KorQuAD/squad_kor_v1\" 데이터셋 구조\n",
        "Hugging Face의 `\"KorQuAD/squad_kor_v1\"`은 **SQuAD 형식을 따른다**.\n",
        "데이터는 크게 `train`과 `validation`으로 구성되어 있다.\n",
        "\n",
        "#### ✔ 데이터셋 크기\n",
        "\n",
        "| 데이터셋 | 샘플 개수 |\n",
        "|----------|-----------|\n",
        "| Train (훈련 데이터) | 약 60,407개 |\n",
        "| Validation (검증 데이터) | 약 5,774개 |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 3. 데이터셋 상세 구조\n",
        "각 데이터 샘플은 다음과 같은 필드를 포함한다.\n",
        "\n",
        "#### ✔ 데이터 예시\n",
        "```\n",
        "{\n",
        "    'id': '654321',  \n",
        "    'title': '세종대왕',\n",
        "    'context': '세종대왕은 조선의 네 번째 왕으로, 한글을 창제한 것으로 유명하다. 그는 1397년에 태어나 1418년에 즉위하였다...',\n",
        "    'question': '세종대왕이 즉위한 연도는?',\n",
        "    'answers': {'text': ['1418년'], 'answer_start': [50]}\n",
        "}\n",
        "```\n",
        "\n",
        "#### ✔ 각 필드 설명\n",
        "\n",
        "- **id**: 질문 샘플의 고유 ID\n",
        "- **title**: 해당 문단의 제목\n",
        "- **context**: 질문에 대한 정답이 포함된 문맥(Paragraph)\n",
        "- **question**: 주어진 `context`를 바탕으로 한 질문\n",
        "- **answers**: 정답 (정답 문자열과 해당 문자열의 시작 위치 포함)\n",
        "  - `answers[\"text\"]`: 실제 정답 텍스트\n",
        "  - `answers[\"answer_start\"]`: `context` 내에서 정답이 시작하는 문자 인덱스\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 4. KorQuAD를 사용하는 이유\n",
        "**한국어 기반 QA 모델을 훈련할 수 있음**\n",
        "**Hugging Face에서 쉽게 로드 가능**\n",
        "**SQuAD 형식과 동일하여 기존 영어 기반 모델을 한국어로 전이 학습하기 좋음**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mbBrgkXeX1lU"
      },
      "id": "mbBrgkXeX1lU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **분석 및 고찰**\n",
        "\n",
        "## **BERT 모델을 사용한 이유**\n",
        "\n",
        "### **1. 사전 학습된 강력한 인코더 기반 모델**\n",
        "BERT(Bidirectional Encoder Representations from Transformers)는 대규모 코퍼스를 활용한 사전 학습을 통해 강력한 문맥적 이해 능력을 갖추고 있다. 이를 통해 QA(Task)에서 높은 성능을 발휘할 수 있다.\n",
        "\n",
        "### **2. 양방향 학습(Bidirectional Context Learning)**\n",
        "BERT는 문장의 양방향 문맥을 동시에 고려하여 단어의 의미를 학습한다. 이는 질문과 문맥(Context) 간의 관계를 보다 정밀하게 이해하는 데 도움이 된다.\n",
        "\n",
        "### **3. 토큰 수준의 정밀한 위치 예측**\n",
        "KorQuAD와 같은 MRC(Task)에서는 정답의 시작과 끝 위치를 예측해야 한다. BERT는 각 토큰의 표현을 효과적으로 학습하며, 이를 통해 높은 정확도로 정답의 위치를 찾아낼 수 있다.\n",
        "\n",
        "### **4. 사전 학습 + 파인튜닝 구조**\n",
        "BERT는 이미 방대한 데이터로 사전 학습된 상태에서 제공되므로, 특정 도메인(예: 한국어 QA)에서 적은 양의 데이터로도 효과적인 파인튜닝이 가능하다.\n",
        "\n",
        "### **5. 한국어 지원 모델(KLUE-BERT)의 존재**\n",
        "KorQuAD 데이터셋을 활용하려면 한국어에 특화된 모델이 필요하다. KLUE-BERT는 한국어 자연어 처리(NLP)를 위해 최적화된 모델로, KorQuAD 태스크에서 강력한 성능을 낼 수 있다.\n",
        "\n",
        "---\n",
        "\n",
        "## **실험 결과 분석**\n",
        "\n",
        "### **1. 학습 진행 과정**\n",
        "- 학습 초기 부터, pre_trained 모델이 너무 잘 만들어 졌는지, 좋은 성능을 볼 수 있다.\n",
        "\n",
        "### **2. 평가 결과 (Evaluation Results)**\n",
        "- 모델이 단순한 질문뿐만 아니라, 복잡한 질문에서도 어느 정도 일반화 능력을 갖추었음을 확인하며, loss curve 또한 잘 떨어짐을 확인.\n",
        "\n",
        "---\n",
        "\n",
        "## **한계점 및 개선 방향**\n",
        "\n",
        "### **1. Answer 생성 문제**\n",
        "- 우리 모델은 기존의 정보를 기억하거나, 학습하려는 시도를 전혀 하지 않기 때문에\n",
        "content 맥락의 여부에 크게 의존한다.\n",
        "\n",
        "### **2. 추론 문제 **\n",
        "- BERT 기반 모델은 전혀 생성하지 않고, 단순히 context 내의 어느 부분이 가장 중요한지 여부를 통해 시작과 끝점을 탐색하는 것이 전부이다. 즉 그렇기에 아예 context에서 추측해야 하는 어려운 문제의 경우는 적합한 답을 이끌어내지 못한다.\n",
        "---\n",
        "\n",
        "## **결론**\n",
        "본 실험을 통해 KLUE-BERT 모델이 KorQuAD 데이터셋에서 효과적으로 학습될 수 있음을 확인하였다. 하지만, Encoder 기반의 모델이기 때문에 항상 Context 내에 정답이 있음을 보장해야 한다는 문제가 있다. 또한, 다양한 한국어 QA 데이터셋을 활용하여 보다 넓은 범위의 질의응답 태스크에 적용할 수 있을 것이다.\n"
      ],
      "metadata": {
        "id": "Eh61mQMIqBLr"
      },
      "id": "Eh61mQMIqBLr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **특이사항**  \n",
        "\n",
        "내가 시도해본 접근 방식은 총 두 가지다.  \n",
        "\n",
        "1. **BERT 기반 모델을 이용해 시작(Start), 끝(End) 토큰 위치 찾기**  \n",
        "2. **Decoder 기반 모델을 이용하여 정답을 직접 생성하기**  \n",
        "\n",
        "결과적으로 1번 방법을 선택하여 과제를 제출했는데, 두 방법의 장단점은 다음과 같다.  \n",
        "\n",
        "---\n",
        "\n",
        "## **1번 방법: BERT 기반 토큰 위치 예측**  \n",
        "\n",
        "### **장점**  \n",
        "- **학습이 비교적 빠름** → 사전 학습된 BERT 모델을 파인튜닝하기 때문에 학습 속도가 상대적으로 빠르다.  \n",
        "- **간편한 구현** → Hugging Face에서 `AutoModelForQuestionAnswering`을 제공하므로 별도의 Head를 직접 구성할 필요 없이 불러와서 사용하면 된다.  \n",
        "- **정확한 정답을 예측** → 정답이 `context` 내에 존재하기만 하면, 높은 정확도로 정답 위치를 찾아낼 수 있다.  \n",
        "\n",
        "### **단점**  \n",
        "- **Context 의존성이 높음** → 정답을 찾기 위해 반드시 `context`가 주어져야 하며, `context` 없이 단독 질문에 대해 답변할 수 없다.  \n",
        "- **추론이 제한적** → 질문이 복잡하거나 `context` 내에서 명확한 정답을 찾기 어려운 경우 성능이 저하될 수 있다.  \n",
        "- **정확한 위치 예측이 필요** → 정답의 시작과 끝 위치를 잘못 예측하면 답변이 부정확해질 수 있다.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2번 방법: Decoder 기반 생성 모델**  \n",
        "\n",
        "### **장점**  \n",
        "- **Context 없이도 답변 가능** → 정답이 `context`에 포함되지 않더라도, 모델이 새로운 답변을 생성할 수 있다.  \n",
        "- **보다 유연한 질의응답** → 단순한 `Span Extraction` 방식이 아니라, 열린 도메인의 질문에도 답할 수 있는 가능성이 있다.  \n",
        "\n",
        "### **단점**  \n",
        "- **학습이 오래 걸림** → 생성 모델(예: T5, GPT)은 일반적으로 학습 파라미터가 많아, 파인튜닝 시 시간이 오래 걸린다.  \n",
        "- **정확성 문제** → `context`에 정답이 존재함에도 불구하고, 생성 과정에서 잘못된 답변을 만들어낼 가능성이 있다.  \n",
        "- **추론 속도가 느림** → 답변을 생성하는 과정에서 디코딩이 필요하므로, `Span Extraction` 방식보다 추론 속도가 느리다.  \n",
        "\n",
        "---\n",
        "\n",
        "## **결론 및 선택 이유**  \n",
        "\n",
        "BERT 기반의 **토큰 위치 예측 방식(1번)**을 선택한 이유는 다음과 같다.  \n",
        "\n",
        "1. **효율적인 학습** → 제한된 시간 내에 최적의 성능을 내기 위해 학습 속도가 빠른 모델을 선택해야 했다.  \n",
        "2. **Hugging Face 지원** → 이미 제공되는 QA 모델 구조를 활용하면, 모델 구조를 직접 수정하지 않고 빠르게 적용할 수 있었다.  \n",
        "3. **높은 신뢰성** → 정답이 `context`에 포함되어 있기만 하면, 비교적 안정적인 성능을 보였다.  \n",
        "4. **추론 속도** → 생성 모델보다 빠르게 예측할 수 있어 실제 응용에도 적합했다.\n",
        "\n",
        "더불어 DLPC 환경에서의 Decoder 기반의 모델 같이 큰 모델을 학습하기에는 중간에 학습이 종료되어 처음부터 다시 돌려야 한다는 등 제약사항이 많아서 결국 BERT 기반의 모델로 제출하게 되었다.\n",
        "\n",
        "하지만, 열린 도메인 질문(예: `\"세계에서 가장 높은 산은?\"`)에 대한 답변을 생성하는 것은 BERT 기반 모델로는 어려웠다. 따라서, 만약 특정 도메인에 한정되지 않은 QA 시스템을 구축해야 한다면, **Decoder 기반 모델**을 고려할 필요가 있다고 생각한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "KEsQ4SRPsblZ"
      },
      "id": "KEsQ4SRPsblZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}