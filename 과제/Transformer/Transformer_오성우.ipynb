{"cells":[{"cell_type":"markdown","source":["#Transformer 과제"],"metadata":{"id":"kCW0A9-_YS_k"},"id":"kCW0A9-_YS_k"},{"cell_type":"code","execution_count":null,"id":"2898a680-e4c7-44b0-9ea5-2095be09d5f3","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:38:18.415100Z","iopub.status.busy":"2025-02-21T10:38:18.414735Z","iopub.status.idle":"2025-02-21T10:38:25.581242Z","shell.execute_reply":"2025-02-21T10:38:25.580063Z","shell.execute_reply.started":"2025-02-21T10:38:18.415071Z"},"scrolled":true,"collapsed":true,"id":"2898a680-e4c7-44b0-9ea5-2095be09d5f3","outputId":"b5c722bb-d0d8-4fca-ded2-039383ca708c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Looking in indexes: http://repo.ai.gato/registry/repository/pypi-proxy/simple\n","Requirement already satisfied: transformers in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (4.48.3)\n","Requirement already satisfied: datasets in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (19.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n","Requirement already satisfied: xxhash in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/fsspec/2024.9.0/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","Requirement already satisfied: aiohttp in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from datasets) (3.11.12)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/gatoai/python/venv/jupyter-4.3.5-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/gatoai/python/venv/pytorch-2.6.0-cuda12.4-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Installing collected packages: fsspec\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.12.0 requires numba-cuda<0.0.18,>=0.0.13, which is not installed.\n","s3fs 2025.2.0 requires fsspec==2025.2.0.*, but you have fsspec 2024.9.0 which is incompatible.\n","cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fsspec-2024.9.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install transformers datasets"]},{"cell_type":"markdown","source":["##1. 라이브러리 임포트 및 데이터셋 로드\n","\n","- 질의 응답에 특화된 사전학습 모델을 불러오기 위해 AutoModelForQuestionAnswering 임포트\n","\n","- 조기 종료를 위한 EarlyStoppingCallback 임포트"],"metadata":{"id":"oefAjHAqYSXA"},"id":"oefAjHAqYSXA"},{"cell_type":"code","execution_count":null,"id":"e2f92afb-ff07-4587-b2f7-b4c04e9819cf","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:49:20.670989Z","iopub.status.busy":"2025-02-21T10:49:20.670619Z","iopub.status.idle":"2025-02-21T10:49:24.052050Z","shell.execute_reply":"2025-02-21T10:49:24.051297Z","shell.execute_reply.started":"2025-02-21T10:49:20.670962Z"},"id":"e2f92afb-ff07-4587-b2f7-b4c04e9819cf"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForQuestionAnswering,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","import torch\n","import numpy as np\n","\n","# KorQuAD 데이터셋 로드\n","dataset = load_dataset(\"KorQuAD/squad_kor_v1\")"]},{"cell_type":"markdown","source":["##2. 데이터셋 전처리\n","\n","- \"xlm-roberta-base\" 모델의 토크나이저 불러옴 (XLM-RoBERTa는 다국어로 학습된 모델로 한국어도 비교적 잘 처리함)"],"metadata":{"id":"ci8_UZ57YwAo"},"id":"ci8_UZ57YwAo"},{"cell_type":"code","execution_count":null,"id":"f4f21e0e-16a8-4251-af0c-653494292eb8","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:49:24.053138Z","iopub.status.busy":"2025-02-21T10:49:24.052919Z","iopub.status.idle":"2025-02-21T10:49:28.182225Z","shell.execute_reply":"2025-02-21T10:49:28.178581Z","shell.execute_reply.started":"2025-02-21T10:49:24.053117Z"},"id":"f4f21e0e-16a8-4251-af0c-653494292eb8"},"outputs":[],"source":["# 모델 체크포인트 지정 및 토크나이저 로드\n","model_checkpoint = \"xlm-roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    contexts = examples[\"context\"]\n","\n","    tokenized_examples = tokenizer(\n","        questions,\n","        contexts,\n","        truncation=\"only_second\",\n","        max_length=384,\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offsets in enumerate(offset_mapping):\n","        sample_index = sample_mapping[i]\n","        answer = examples[\"answers\"][sample_index]\n","\n","        if len(answer[\"answer_start\"]) == 0:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            start_char = answer[\"answer_start\"][0]\n","            end_char = start_char + len(answer[\"text\"][0])\n","\n","            sequence_ids = tokenized_examples.sequence_ids(i)\n","            context_start = sequence_ids.index(1)\n","            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n","\n","            token_start_index = context_start\n","            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                token_start_index += 1\n","            token_start_index -= 1\n","\n","            token_end_index = context_end\n","            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n","                token_end_index -= 1\n","            token_end_index += 1\n","\n","            start_positions.append(token_start_index)\n","            end_positions.append(token_end_index)\n","\n","    tokenized_examples[\"start_positions\"] = start_positions\n","    tokenized_examples[\"end_positions\"] = end_positions\n","    return tokenized_examples\n","\n","tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"]},{"cell_type":"markdown","source":["##3. 모델 로드 및 평가 지표 정의"],"metadata":{"id":"l5RsNBulZzYH"},"id":"l5RsNBulZzYH"},{"cell_type":"code","execution_count":null,"id":"6cd06726-b623-4707-8355-4c39121bad6a","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:49:28.186668Z","iopub.status.busy":"2025-02-21T10:49:28.185114Z","iopub.status.idle":"2025-02-21T10:49:28.561144Z","shell.execute_reply":"2025-02-21T10:49:28.560504Z","shell.execute_reply.started":"2025-02-21T10:49:28.186642Z"},"id":"6cd06726-b623-4707-8355-4c39121bad6a","outputId":"d6c0e1a5-6fbd-4ca6-910d-b1d490ab964d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    start_logits, end_logits = predictions\n","    start_preds = np.argmax(start_logits, axis=-1)\n","    end_preds = np.argmax(end_logits, axis=-1)\n","\n","    if isinstance(labels, tuple):\n","        start_labels, end_labels = labels\n","    else:\n","        start_labels = labels[:, 0]\n","        end_labels = labels[:, 1]\n","\n","    exact_matches = (start_preds == start_labels) & (end_preds == end_labels)\n","    accuracy = np.mean(exact_matches.astype(np.float32))\n","    return {\"accuracy\": accuracy}"]},{"cell_type":"markdown","source":["##4. TrainingArguments 및 Trainer 생성\n","\n","- Gradient Accumulation\n","\n","  - GPU 메모리가 제한적일 때, 한 번의 실제 파라미터 업데이트(optimizer.step()) 전에 여러 배치를 순차적으로 Forward/Backward하여 기울기를 누적하는 방식.\n","  - 예: gradient_accumulation_steps=2라면, batch size 8의 데이터를 2회 forward/backward 후 최종적으로 batch size 16에 해당하는 gradient를 한꺼번에 업데이트함.\n","  - 이는 메모리를 효율적으로 사용하면서 효과적으로 대형 배치 사이즈로 학습한 것과 유사한 효과를 낼 수 있습니다.\n","\n","- Mixed Precision Training(fp16=True)\n","\n","  - 32-bit 부동소수점 대신 16-bit 부동소수점을 활용해 연산을 가속하고, 메모리 사용량도 절감.\n","  - NVIDIA GPU(예: Volta, Turing, Ampere 아키텍처)에서 자동 혼합 정밀도(Amp)를 사용하면 손쉽게 적용 가능.\n","  - 성능(학습 속도) 개선과 함께 GPU 메모리 사용 효율이 높아집니다.\n","\n","- Warm-up\n","\n","  - 학습 초반에 학습률을 너무 높게 설정하면 모델이 불안정하게 수렴할 수 있음.\n","  - warmup_steps=500을 통해 처음 500 스텝 동안은 학습률을 점차 높여 안정적인 학습이 가능하게 함.\n","- EarlyStoppingCallback\n","\n","  - 검증 손실(eval_loss)이 개선되지 않으면 미리 학습을 멈춰서 오버피팅을 방지.\n","  - early_stopping_patience=2는 개선이 없으면 2번 시점 이후 정지하겠다는 의미.\n","\n","- Best Model Load\n","\n","  - load_best_model_at_end=True 옵션으로 학습이 끝나면 eval_loss가 가장 좋은(낮은) 체크포인트로 로드.\n","  - 가장 성능이 좋은 모델 파라미터를 최종 결과로 사용할 수 있음."],"metadata":{"id":"WOHPQVi_aHU5"},"id":"WOHPQVi_aHU5"},{"cell_type":"code","execution_count":null,"id":"7dfc167c-6a34-4a7f-af20-d21150787891","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:49:28.568941Z","iopub.status.busy":"2025-02-21T10:49:28.567706Z","iopub.status.idle":"2025-02-21T10:49:28.971924Z","shell.execute_reply":"2025-02-21T10:49:28.970665Z","shell.execute_reply.started":"2025-02-21T10:49:28.568838Z"},"id":"7dfc167c-6a34-4a7f-af20-d21150787891","outputId":"651f0ed8-8421-4243-c691-cf414cb6b5f0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/tmp/ipykernel_6129/929676005.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    learning_rate=3e-5,\n","    weight_decay=0.01,\n","    logging_steps=100,\n","    save_steps=500,\n","    gradient_accumulation_steps=2,  # Gradient Accumulation 적용\n","    fp16=True,                     # Mixed Precision Training 적용\n","    warmup_steps=500,              # Warm-up 단계 추가\n","    load_best_model_at_end=True,   # 최적 모델 저장 활성화\n","    metric_for_best_model=\"eval_loss\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",")"]},{"cell_type":"markdown","source":["##5. 학습 진행 및 평가"],"metadata":{"id":"zWfntZzAatxM"},"id":"zWfntZzAatxM"},{"cell_type":"code","execution_count":null,"id":"53bc4706-de54-445a-84d9-1aa0aa1277a2","metadata":{"execution":{"iopub.execute_input":"2025-02-21T10:49:28.975415Z","iopub.status.busy":"2025-02-21T10:49:28.974159Z","iopub.status.idle":"2025-02-21T11:14:57.234868Z","shell.execute_reply":"2025-02-21T11:14:57.234280Z","shell.execute_reply.started":"2025-02-21T10:49:28.975389Z"},"id":"53bc4706-de54-445a-84d9-1aa0aa1277a2","outputId":"468069e7-9b4f-448b-ca94-1c73aee72c3c"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5000' max='13686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 5000/13686 25:03 < 43:33, 3.32 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.304500</td>\n","      <td>1.200222</td>\n","      <td>0.633657</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.929700</td>\n","      <td>0.854586</td>\n","      <td>0.713837</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.779900</td>\n","      <td>0.707701</td>\n","      <td>0.745390</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.717100</td>\n","      <td>0.731240</td>\n","      <td>0.761781</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.625700</td>\n","      <td>0.620131</td>\n","      <td>0.788007</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.624700</td>\n","      <td>0.600621</td>\n","      <td>0.792651</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.658400</td>\n","      <td>0.563525</td>\n","      <td>0.796886</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.571500</td>\n","      <td>0.555889</td>\n","      <td>0.798115</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.598200</td>\n","      <td>0.565315</td>\n","      <td>0.795929</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.446600</td>\n","      <td>0.566446</td>\n","      <td>0.808223</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='916' max='916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [916/916 00:23]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluation results: {'eval_loss': 0.5558894872665405, 'eval_accuracy': 0.7981150150299072, 'eval_runtime': 23.891, 'eval_samples_per_second': 306.434, 'eval_steps_per_second': 38.341, 'epoch': 1.0960105217010083}\n"]}],"source":["# 학습 시작\n","trainer.train()\n","\n","# 평가 진행 (loss 및 accuracy 출력)\n","eval_results = trainer.evaluate()\n","print(\"Evaluation results:\", eval_results)\n"]},{"cell_type":"markdown","source":["##6. 테스트"],"metadata":{"id":"5H3I_SNVazsM"},"id":"5H3I_SNVazsM"},{"cell_type":"code","execution_count":null,"id":"f6b6cba1-0fcb-494b-865a-e0fcc853f1b6","metadata":{"execution":{"iopub.execute_input":"2025-02-21T11:32:25.684212Z","iopub.status.busy":"2025-02-21T11:32:25.683883Z","iopub.status.idle":"2025-02-21T11:32:25.753940Z","shell.execute_reply":"2025-02-21T11:32:25.753323Z","shell.execute_reply.started":"2025-02-21T11:32:25.684188Z"},"id":"f6b6cba1-0fcb-494b-865a-e0fcc853f1b6","outputId":"90f081b3-8a49-401d-f881-deef755f428d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Answer: 2020년 1월 20일에\n"]}],"source":["def predict(question, context):\n","    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=384)\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    start_logits = outputs.start_logits\n","    end_logits = outputs.end_logits\n","    start_index = torch.argmax(start_logits, dim=1).item()\n","    end_index = torch.argmax(end_logits, dim=1).item()\n","    answer_ids = inputs[\"input_ids\"][0][start_index : end_index + 1]\n","    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n","    return answer\n","\n","test_question = \"코로나19의 첫 번째 확진자는 언제 나왔나요?\"\n","test_context = \"대한민국의 첫 번째 코로나19 확진자는 2020년 1월 20일에 확인된 환자입니다.\"\n","print(\"Predicted Answer:\", predict(test_question, test_context))"]},{"cell_type":"markdown","source":["##어려웠던 점\n","- 데이터셋이 스팬 추출 기반의 질의응답이었는데 원본 텍스트 기준 문자 위치를 토큰 기준 위치로 매핑해야했다. 이 offset_mapping 로직이 복잡하여서 실수가 많아 구현하는데 오래걸렸다.\n","\n","- 학습에 몇 시간이 넘게 소요되는 모델과 관련한 과제를 처음 진행해봐서 중간에 런타임이 끊기는 등 처음엔 제대로 시작조차 하지 못하였다. 크지 않는 모델을 사용하고 Early Stopping을 적용한 이후에는 그래도 학습이 비교적 빠르게 끝났다."],"metadata":{"id":"wD7Xnr6mbCKF"},"id":"wD7Xnr6mbCKF"},{"cell_type":"markdown","source":["##분석 및 고찰\n","- 한국어에 특화된 모델인 KoBERT같은 모델을 나중 가서 알게 되어서 이걸로 구현을 해봤으면 더 좋지 않았을까 생각한다.\n","\n","- GPU 자원이 한정되어있어 CS 11강에서 배웠던 하이퍼 파라미터 튜닝 기법을 적용시켜 보았는데 꽤 괜찮은 결과가 나온 것 같다."],"metadata":{"id":"lLDsLjCObusH"},"id":"lLDsLjCObusH"}],"metadata":{"kernelspec":{"display_name":"PyTorch 2.6.0 (py3.10)","language":"python","name":"pytorch-2.6.0-cuda12.4-py3.10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}